from transformers import AutoModel, AutoTokenizer, pipeline, AutoModelForCausalLM
import torch
import numpy as np
from pooling import chunked_pooling
from chunking import Chunker
from typing import List, Tuple, Union, Dict
from tqdm import tqdm
import requests
import os
import sys
import time
from qdrant_client.models import PointStruct, VectorParams, Distance, OptimizersConfigDiff
import pickle
from datetime import datetime
import uuid
import hashlib
class LateChunkingEmbedder:

    def __init__(self, 
            model: AutoModel,
            tokenizer: AutoTokenizer, 
            chunking_strategy: str = "sentences",
            n_sentences: int = 1,
            embedding_model_name: str = None,
            device: str = "cuda",
            max_length: int = 2000
        ):
        self.max_length = max_length
        self.device = device
        self.model = model.to(self.device)
        self.tokenizer = tokenizer
        self.chunker = Chunker(chunking_strategy = chunking_strategy, embedding_model_name=embedding_model_name)
        self.n_sentences = n_sentences

    
    # ë‹¨ì¼ ë¬¸ì„œë¥¼ í…ŒìŠ¤íŠ¸í•  ë•Œë§Œ ì‚¬ìš©
    def run(self, document: str):
        annotations = [self.chunker.chunk(text=document, tokenizer=self.tokenizer, n_sentences=self.n_sentences)]
        model_inputs = self.tokenizer(
            document,
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=self.max_length,
        )
        model_inputs = {k: v.to("cuda") for k, v in model_inputs.items()}
                       
        # Forward pass
        with torch.no_grad():
            model_outputs = self.model(**model_inputs)
        
        self.output_embs = chunked_pooling(
            model_outputs, annotations, max_length=self.max_length, 
        )[0]
        return self.output_embs

    

    '''
    qdrantì— ì ì¬ ì‹œ distanceë¥¼ cosineìœ¼ë¡œ í•˜ëŠ” ê²½ìš° embedding normalization í•„ìš”í•˜ë‚˜ qdrantì—ì„œ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë¨
    '''
    def upload_to_qdrant(
        self,
        documents: List[str],
        qdrant_client,
        collection_name: str,
        batch_size: int = 8
    ):  
        results = self.batch_embed_with_chunks(documents, batch_size=batch_size)

        if not qdrant_client.collection_exists(collection_name):
            print("embedding dim : ", results[0]["embedding"].shape[0])
            qdrant_client.create_collection(collection_name=collection_name, 
                                            vectors_config=VectorParams(size=results[0]["embedding"].shape[0]
                                                                        , distance=Distance.COSINE
                                                                        , on_disk=True),
                                            optimizers_config=OptimizersConfigDiff(indexing_threshold=100, 
                                                                            )
                                                                                )
                                                                                                           
                                                                                                                                                                          
            print(f"âœ… Created collection : '{collection_name}'/n")
        
        

        BATCH_SIZE = 50  # ë˜ëŠ” 50ê°œ ì •ë„ë¡œ ì¤„ì´ê¸°

        for i in tqdm(range(0, len(results), BATCH_SIZE)): # tqdm ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì¶”ê°€
            batch = results[i:i + BATCH_SIZE]

            points = [
                PointStruct(
                    id=str(uuid.uuid4()),
                    vector=doc["embedding"].tolist(),
                    payload={
                        "text": doc["text"],
                        "doc_id": doc["doc_id"],
                        "chunk_index": doc["chunk_index"],
                        "published_date" : doc["published_date"]
                    }
                )
                for doc in batch
            ]

            qdrant_client.upsert(collection_name=collection_name, points=points, wait=True)

        print(f"âœ… Uploaded {len(points)} points to Qdrant collection '{collection_name}'")

    
    def deduplicate_chunks_by_text(self, chunks): # chunks : List[Dict[str, Union[np.ndarray, str, int]]]
        print("ğŸ” Removing duplicate chunks...")
        print("Original chunks : ", len(chunks))
        seen_hashes = set()
        deduped = []
        for c in chunks:
            h = hashlib.md5(c["text"].strip().encode()).hexdigest()
            if h not in seen_hashes:
                deduped.append(c)
                seen_hashes.add(h)
        print("Deduplicated chunks : ", len(deduped), "/n")
        return deduped    
    
    
    '''
    ëª¨ë¸ì´ ì´ë¯¸ GPUì— ì˜¬ë¼ê°„ ê²½ìš°ëŠ”, ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹ì´ ê°€ì¥ íš¨ìœ¨ì ì´ê³  ì•ˆì •ì .
    ProcessPoolExecutorëŠ” GPU ì‚¬ìš© ë¶ˆê°€í•œ CPU ì „ìš© ì‘ì—…ì—ëŠ” ì¢‹ì§€ë§Œ, 
    GPU ì‚¬ìš© ê°€ëŠ¥í•œ ì‘ì—…ì—ëŠ” ì†ë„ê°€ ëŠë¦¬ë¯€ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ.
    '''
    def batch_embed_with_chunks(
        self,
        documents: List[str],
        batch_size: int = 8
    ) -> List[Dict[str, Union[np.ndarray, str, int]]]:
        """
        ë¬¸ì„œë¥¼ ë°°ì¹˜ë¡œ ì„ë² ë”©í•˜ê³  ê° ì²­í¬ì˜ í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ë°˜í™˜

        Returns: List of dicts:
            {
                "embedding": np.ndarray,
                "text": str,
                "doc_id": int,
                "chunk_index": int
            }
        """
        results = []

        for i in tqdm(range(0, len(documents), batch_size), desc="Batch embedding"):
            batch_docs = documents[i:i + batch_size]
            batch_docs_only_contents = [f'{doc["title"]} {doc["content"]}' for doc in batch_docs]
            

            # 1. ì²­í¬ ì–´ë…¸í…Œì´ì…˜
            # doc : dict
            annotations_batch = [
                self.chunker.chunk(text=f'{doc["title"]} {doc["content"]}', tokenizer=self.tokenizer, n_sentences=self.n_sentences)
                for doc in batch_docs
            ]

            # 2. ëª¨ë¸ ì¸í¼ëŸ°ìŠ¤ë¥¼ ìœ„í•œ tokenizer
            model_inputs = self.tokenizer(
                batch_docs_only_contents,
                return_tensors='pt',
                padding=True,
                truncation=True,
                max_length=self.max_length,
            )
            model_inputs = {k: v.to(self.device) for k, v in model_inputs.items()}

            # 3. ëª¨ë¸ ì¸í¼ëŸ°ìŠ¤
            with torch.no_grad():
                model_outputs = self.model(**model_inputs)

            # 4. ì²­í¬ë‹¹ ì„ë² ë”© ê³„ì‚°
            batch_embs = chunked_pooling(model_outputs, annotations_batch, max_length=self.max_length)

            # 5. ê° ì²­í¬ í…ìŠ¤íŠ¸ ë³µì› ë° ê²°ê³¼ ê²°í•©
            for doc_idx, (doc, annotations, chunk_embs) in enumerate(zip(batch_docs, annotations_batch, batch_embs)):
                full_text = f'{doc["title"]} {doc["content"]}'
                doc_inputs = self.tokenizer(full_text, return_tensors='pt', truncation=True, max_length=self.max_length)
                input_ids = doc_inputs["input_ids"][0]  # shape: (seq_len,)

                for chunk_idx, ((start_idx, end_idx), emb) in enumerate(zip(annotations, chunk_embs)):
                    chunk_tokens = input_ids[start_idx:end_idx]
                    chunk_text = self.tokenizer.decode(chunk_tokens, skip_special_tokens=True)

                    results.append({
                        "embedding": emb,
                        "text": chunk_text,
                        "doc_id": i + doc_idx,
                        "chunk_index": chunk_idx,
                        "published_date": doc["publishDateTime"]
                    })

        # ì¤‘ë³µ ì œê±° 
        deduped_results = self.deduplicate_chunks_by_text(results)
        
        # ì—ëŸ¬ ëŒ€ë¹„ ê²°ê³¼ì €ì¥
        today = datetime.now().strftime("%Y-%m-%d")
        with open(f'./results_{today}.pkl', 'wb') as f:
            pickle.dump(deduped_results, f)
        return deduped_results

  
   


    def query(self, query: str):
        if "output_embs" not in dir(self):
            raise ValueError("no embeddings calculated, use .run(document) to create chunk embeddings")
        
        model_inputs = self.tokenizer(
            query,
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=self.max_length,
        )
        
        model_inputs = {k: v.to(self.device) for k, v in model_inputs.items()}

        with torch.no_grad():
            query_embedding = self.model(**model_inputs)

        query_embedding = query_embedding.last_hidden_state[:, 0, :].detach().cpu().numpy()  
        
        similarities = []
        for emb in self.output_embs:
            emb = emb.reshape(1, -1)
            cos_sim = cosine_similarity(emb, query_embedding)
            similarities.append(cos_sim)
        
        return similarities

def cosine_similarity(vector1, vector2):
    vector1_norm = vector1 / np.linalg.norm(vector1)
    vector2_norm = vector2 / np.linalg.norm(vector2)
    # cosine ìœ ì‚¬ë„ ê³„ì‚°
    return np.sum(vector1_norm * vector2_norm, axis=-1)